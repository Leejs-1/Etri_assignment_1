{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 이해 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_PATH = './DATA/' #데이터경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table(DATA_PATH + 'ratings_train.txt')\n",
    "test_data = pd.read_table(DATA_PATH + 'ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정 리뷰 갯수: 74827\n",
      "부정 리뷰 갯수: 75173\n"
     ]
    }
   ],
   "source": [
    "#긍정 1, 부정 0\n",
    "print('긍정 리뷰 갯수: {}'.format(train_data['label'].value_counts()[1]))\n",
    "print('부정 리뷰 갯수: {}'.format(train_data['label'].value_counts()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  아 더빙.. 진짜 짜증나네요 목소리\n",
       "1                    흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
       "2                                    너무재밓었다그래서보는것을추천한다\n",
       "3                        교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n",
       "4    사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...\n",
       "Name: document, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['document'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    아 더빙 진짜 짜증나네요 목소리\n",
       "1                           흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나\n",
       "2                                    너무재밓었다그래서보는것을추천한다\n",
       "3                            교도소 이야기구먼 솔직히 재미는 없다평점 조정\n",
       "4    사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...\n",
       "Name: document, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한글과 공백을 제외하고 모두 제거\n",
    "train_data['document'] = train_data['document'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','')\n",
    "train_data['document'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id             0\n",
      "document    1260\n",
      "label          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "train_data['document'].replace('', np.nan, inplace=True) # 리뷰에 비어있으면 NULL 값으로 대체\n",
    "# NULL값이 존재하는 행이 있는지 확인\n",
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>4221289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>9509970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>10147571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>7117896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>6478189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id document  label\n",
       "404   4221289      NaN      0\n",
       "412   9509970      NaN      1\n",
       "470  10147571      NaN      1\n",
       "584   7117896      NaN      0\n",
       "593   6478189      NaN      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NULL값이 존재하는 5개의 행 출력\n",
    "train_data.loc[train_data.document.isnull()][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 train 데이터의 개수:  148740\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.dropna(how = 'any')\n",
    "print('전처리 후 train 데이터의 개수: ', len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 test 데이터의 개수 : 49575\n"
     ]
    }
   ],
   "source": [
    "# train 데이터와 마찬가지로 test 데이터 전처리\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "test_data['document'] = test_data['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
    "test_data['document'].replace('', np.nan, inplace=True) # empty은 NULL 값으로 변경\n",
    "test_data = test_data.dropna(how='any') # NULL 값 제거\n",
    "print('전처리 후 test 데이터의 개수 :',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 정의\n",
    "stopwords = ['은','는','이','가','하','아','것','들','의','있','되',\n",
    "             '수','보','주','등','한', '좀', '잘', '걍', '과', '도',\n",
    "             '을', '를', '으로', '자', '에', '와', '하다']\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 okt를 통해 토근화를 하면서 불용어 제거 후 X_train에 저장\n",
    "X_train = []\n",
    "for sentence in train_data['document']:\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['더빙', '진짜', '짜증나다', '목소리'], ['흠', '포스터', '보고', '초딩', '영화', '줄', '오버', '연기', '조차', '가볍다', '않다'], ['너', '무재', '밓었', '다그', '래서', '보다', '추천', '다'], ['교도소', '이야기', '구먼', '솔직하다', '재미', '없다', '평점', '조정']]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터와 마찬가지로 test 데이터 토큰화 및 불용어 제거\n",
    "X_test = []\n",
    "for sentence in test_data['document']:\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기계가 텍스트를 숫자로 처리할 수 있도록 train데이터에 정수 인코딩 수행, 먼저 train데이터에 대해 단어 집합 생성\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\n",
    "# 0번 패딩 토큰을 고려하여 + 1\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 케라스 토크나이저의 인자로 넘긴 후 텍스트 시퀀스를 정수 시퀀스로 변환 (0 ~ 19416)\n",
    "tokenizer = Tokenizer(vocab_size) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[451, 15, 255, 652], [911, 453, 39, 592, 1, 209, 1447, 23, 957, 668, 18], [381, 2444, 2311, 5668, 2, 216, 8], [6492, 100, 8125, 214, 53, 3, 24, 3609]]\n"
     ]
    }
   ],
   "source": [
    "# 변환 됐는지 확인\n",
    "print(X_train[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148474\n",
      "148474\n"
     ]
    }
   ],
   "source": [
    "#빈 샘플 제거\n",
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
    "\n",
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 68\n",
      "리뷰의 평균 길이 : 10.338106335115912\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 서로다른 길이의 리뷰들의 길이를 동일하게 맞춰주는 패딩 작업 진행\n",
    "print('리뷰의 최대 길이 :',max(len(review) for review in X_train))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
    "plt.hist([len(review) for review in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "    count = 0\n",
    "    for sentence in nested_list:\n",
    "        if(len(sentence) <= max_len):\n",
    "            count = count + 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 50 이하인 샘플의 비율: 99.96026240284495\n"
     ]
    }
   ],
   "source": [
    "max_len = 50\n",
    "below_threshold_len(max_len, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,  911,  453,   39,  592,    1,\n",
       "        209, 1447,   23,  957,  668,   18], dtype=int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " [_Derived_]  Fail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential/lstm/StatefulPartitionedCall]]\n\t [[gradient_tape/sequential/embedding/embedding_lookup/Reshape/_34]] [Op:__inference_train_function_3294]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ba16cee410ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m:  [_Derived_]  Fail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential/lstm/StatefulPartitionedCall]]\n\t [[gradient_tape/sequential/embedding/embedding_lookup/Reshape/_34]] [Op:__inference_train_function_3294]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "# 다 대 일 구조의 LSTM 모델 사용, 이진 분류 문제 수행\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_units = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(LSTM(hidden_units))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=5, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "    print(new_sentence)\n",
    "    new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "    print(new_sentence)\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "    print(new_sentence)\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "    print('encoded = ', encoded)\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "    print('pad_new = ', pad_new)\n",
    "    score = float(loaded_model.predict(pad_new)) # 예측\n",
    "    if(score > 0.5):\n",
    "        print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
    "    else:\n",
    "        print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감성 분석할 문장을 입력하시오: 과연 내 인생영화\n",
      "92.39% 확률로 긍정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_predict(input('감성 분석할 문장을 입력하시오: '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
